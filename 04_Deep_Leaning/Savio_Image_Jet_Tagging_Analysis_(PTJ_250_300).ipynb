{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "id": "cABJZKBQnxhI",
    "outputId": "ab4eda96-ac64-422c-a7a6-597cee010151"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWCYTG9XnbK_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical # For y values\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCVURZJ9nbLM"
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wInQv3TNnbLT"
   },
   "source": [
    "## Some Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztstabDLnbLW"
   },
   "outputs": [],
   "source": [
    "def print_heatmap(data):\n",
    "    heatmap = sns.heatmap(data)\n",
    "    heatmap.set(xlabel='Pseudorapidity', ylabel='Azimuthal Angle')\n",
    "    print(heatmap)\n",
    "    #plt.imshow(data, cmap='jet', interpolation='nearest')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GmFWJpU9nbLb"
   },
   "outputs": [],
   "source": [
    "def get_predicted_values(output_values):\n",
    "    predicted_values = []\n",
    "    for probability in output_values:\n",
    "        if probability[0] > probability[1]:\n",
    "            predicted_values.append(0)\n",
    "        else:\n",
    "            predicted_values.append(1)\n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjKYMo_KnbLf"
   },
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M20RSFqSnbLg"
   },
   "source": [
    "First we read the Signal Data and produce a heatmap from the average of all lines. We do so, in order to get the feeling of what our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWm-N06WnbMY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "      <th>623</th>\n",
       "      <th>624</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...    616  617  618  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0  0.0  0.0   \n",
       "\n",
       "   619  620  621  622  623  624  class  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0      1  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0      1  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0      0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0      0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0      0  \n",
       "\n",
       "[5 rows x 626 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.read_csv('full_data.csv', )\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22596, 626)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11298"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data['class'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ge_MLD_nbMc"
   },
   "source": [
    "Now let's examine how our data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s8esvBX-nbMh"
   },
   "source": [
    "# Defining and Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNpOe50onbMi"
   },
   "source": [
    "Split the full dataset into _test_ and _train_ data in a 70-30% rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pPpuYleJnbMj",
    "outputId": "f9374b9a-e861-46d5-fc04-a4ebc6b7d30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15817, 626)\n",
      "(6779, 626)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(full_data, test_size = 0.30)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkBBwObWnbMl"
   },
   "source": [
    "The _Keras_ framework, in order to train its network must receive the dependent and independent variables in  separated tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15817 625\n",
      "(6779, 625)\n",
      "(6779, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15817, 625, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data.iloc[:, :-1]\n",
    "img_rows = X_train.shape[0]\n",
    "img_cols = X_train.shape[1]\n",
    "input_shape = (img_rows, img_cols,1)\n",
    "Y_train = train_data.iloc[:,-1:]\n",
    "print(img_rows, img_cols)\n",
    "X_test = test_data.iloc[:, :-1]\n",
    "print(X_test.shape)\n",
    "Y_test = test_data.iloc[:,-1:]\n",
    "print(Y_test.shape)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "X_test_scaled = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HkCO2uPhnbMm",
    "outputId": "ddf27960-5aae-4b09-989b-2b1fd0d22778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15817, 25, 25, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.asarray(X_train_scaled).reshape(X_train_scaled.shape[0], 25, 25, 1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6779, 25, 25, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test= np.asarray(X_test_scaled).reshape(X_test_scaled.shape[0], 25, 25, 1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 23, 23, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 21, 21, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               147584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 278,274\n",
      "Trainable params: 278,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./melhor_modelo.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True, mode='max', save_weights_only=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(25, 25, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-iHnkB2nbMs"
   },
   "source": [
    "We must convert the dependent variable to be a probability distribution of the possible outcomes, thus, a value of output `1` must become the probabilities `(0, 1)`. Conversely, a `0` outcome value must become the pair `(1, 0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjeHiPhhnbMt"
   },
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train.values, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Tjr8XEjnbMw"
   },
   "source": [
    "## Neural Network Architecture Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APGfrj38nbMw"
   },
   "source": [
    "We defined a simple NN, with only two hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nslkXkT3nbM0"
   },
   "source": [
    "After defining the NN architecture we train it using the `fit` method. We trained it for 50 epochs (backpropagation cycles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12653 samples, validate on 3164 samples\n",
      "Epoch 1/10\n",
      "12640/12653 [============================>.] - ETA: 0s - loss: 0.5871 - accuracy: 0.6992\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.72345, saving model to ./melhor_modelo.hdf5\n",
      "12653/12653 [==============================] - 19s 1ms/sample - loss: 0.5871 - accuracy: 0.6992 - val_loss: 0.5478 - val_accuracy: 0.7235\n",
      "Epoch 2/10\n",
      "12608/12653 [============================>.] - ETA: 0s - loss: 0.5349 - accuracy: 0.7406\n",
      "Epoch 00002: val_accuracy improved from 0.72345 to 0.74716, saving model to ./melhor_modelo.hdf5\n",
      "12653/12653 [==============================] - 17s 1ms/sample - loss: 0.5354 - accuracy: 0.7402 - val_loss: 0.5214 - val_accuracy: 0.7472\n",
      "Epoch 3/10\n",
      "12608/12653 [============================>.] - ETA: 0s - loss: 0.5221 - accuracy: 0.7517\n",
      "Epoch 00003: val_accuracy improved from 0.74716 to 0.75253, saving model to ./melhor_modelo.hdf5\n",
      "12653/12653 [==============================] - 18s 1ms/sample - loss: 0.5226 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7525\n",
      "Epoch 4/10\n",
      "12640/12653 [============================>.] - ETA: 0s - loss: 0.5069 - accuracy: 0.7599\n",
      "Epoch 00004: val_accuracy improved from 0.75253 to 0.75316, saving model to ./melhor_modelo.hdf5\n",
      "12653/12653 [==============================] - 18s 1ms/sample - loss: 0.5070 - accuracy: 0.7597 - val_loss: 0.5040 - val_accuracy: 0.7532\n",
      "Epoch 5/10\n",
      "12640/12653 [============================>.] - ETA: 0s - loss: 0.5009 - accuracy: 0.7675\n",
      "Epoch 00005: val_accuracy improved from 0.75316 to 0.76169, saving model to ./melhor_modelo.hdf5\n",
      "12653/12653 [==============================] - 18s 1ms/sample - loss: 0.5010 - accuracy: 0.7675 - val_loss: 0.4999 - val_accuracy: 0.7617\n",
      "Epoch 6/10\n",
      "12640/12653 [============================>.] - ETA: 0s - loss: 0.4905 - accuracy: 0.7688\n",
      "Epoch 00006: val_accuracy did not improve from 0.76169\n",
      "12653/12653 [==============================] - 18s 1ms/sample - loss: 0.4903 - accuracy: 0.7688 - val_loss: 0.5042 - val_accuracy: 0.7535\n",
      "Epoch 7/10\n",
      "12640/12653 [============================>.] - ETA: 0s - loss: 0.4803 - accuracy: 0.7750\n",
      "Epoch 00007: val_accuracy did not improve from 0.76169\n",
      "12653/12653 [==============================] - 18s 1ms/sample - loss: 0.4801 - accuracy: 0.7752 - val_loss: 0.5025 - val_accuracy: 0.7573\n",
      "Epoch 8/10\n",
      "12608/12653 [============================>.] - ETA: 0s - loss: 0.4769 - accuracy: 0.7799\n",
      "Epoch 00008: val_accuracy did not improve from 0.76169\n",
      "12653/12653 [==============================] - 18s 1ms/sample - loss: 0.4767 - accuracy: 0.7799 - val_loss: 0.5020 - val_accuracy: 0.7582\n",
      "Epoch 9/10\n",
      "12608/12653 [============================>.] - ETA: 0s - loss: 0.4676 - accuracy: 0.7817\n",
      "Epoch 00009: val_accuracy improved from 0.76169 to 0.76485, saving model to ./melhor_modelo.hdf5\n",
      "12653/12653 [==============================] - 18s 1ms/sample - loss: 0.4678 - accuracy: 0.7819 - val_loss: 0.5064 - val_accuracy: 0.7649\n",
      "Epoch 10/10\n",
      "12608/12653 [============================>.] - ETA: 0s - loss: 0.4625 - accuracy: 0.7913\n",
      "Epoch 00010: val_accuracy improved from 0.76485 to 0.76549, saving model to ./melhor_modelo.hdf5\n",
      "12653/12653 [==============================] - 18s 1ms/sample - loss: 0.4620 - accuracy: 0.7916 - val_loss: 0.5052 - val_accuracy: 0.7655\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                      Y_train,\n",
    "                      #batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=[checkpoint],\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v-ro8rCFnbM4"
   },
   "source": [
    "After training the model we need to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dvv63ALpnbM5"
   },
   "outputs": [],
   "source": [
    "Y_test = to_categorical(Y_test.values, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fIZ53SWGnbM7",
    "outputId": "d9b18f4e-380a-4f23-a430-b0734220f9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "[0.4745970973576007, 0.7858091]\n"
     ]
    }
   ],
   "source": [
    "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(model.metrics_names)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cd3XlIacnbM-"
   },
   "source": [
    "Now let's take a look at the missclassified observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38yOmrIunbM_"
   },
   "outputs": [],
   "source": [
    "Y_test_predicted = model.predict(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "pyZPLUptnbNB",
    "outputId": "baa59420-44ff-46ec-bb37-2b94633acea1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7040813 , 0.29591876],\n",
       "       [0.9477712 , 0.05222877],\n",
       "       [0.02068555, 0.9793145 ],\n",
       "       ...,\n",
       "       [0.9596773 , 0.04032274],\n",
       "       [0.04797025, 0.9520297 ],\n",
       "       [0.3260052 , 0.6739947 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "neg9m3kfnbNE"
   },
   "source": [
    "Now we calculate the false negatives and also the false positives by comparing the true value with the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "4RU2WEpGnbNE",
    "outputId": "388984b9-aa0e-4118-bbcb-64fdaaf51751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "[0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "False Positive Rate: 0.12\n",
      "False Negative Rate: 0.09\n"
     ]
    }
   ],
   "source": [
    "Y_test_predicted_values = get_predicted_values(Y_test_predicted)\n",
    "Y_test_values = get_predicted_values(Y_test)\n",
    "print(Y_test_predicted_values[1:10])\n",
    "print(Y_test_values[1:10])\n",
    "\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "for i in range(len(Y_test_values)):\n",
    "    if Y_test_values[i] == 0 and Y_test_predicted_values[i] == 1:\n",
    "        false_positives.append(i)\n",
    "    elif Y_test_values[i] == 1 and Y_test_predicted_values[i] == 0:\n",
    "        false_negatives.append(i)\n",
    "        \n",
    "print(\"False Positive Rate: {:.2f}\".format(len(false_positives)/len(Y_test_values)))\n",
    "print(\"False Negative Rate: {:.2f}\".format(len(false_negatives)/len(Y_test_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWWDZP1NnbNH"
   },
   "source": [
    "And try to visualize the heatmaps for false positives and negatives. Firstly the false negatives, where the network was supposed to answer _Signal_, but instead, it answered _Background_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "CGDjCVk3nbNI",
    "outputId": "74667460-8a60-4c61-c3c3-ab0b641a8e7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 45, 57, 61, 65, 81, 94, 106, 113]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ea6ac464e58c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfalse_negatives\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfalse_negatives_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfalse_negatives\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfalse_negatives_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmean_false_negatives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfalse_negatives_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "print(false_negatives[1:10])\n",
    "false_negatives_values = X_test.iloc[false_negatives,]\n",
    "print(false_negatives_values.shape)\n",
    "\n",
    "mean_false_negatives = false_negatives_values.mean().values\n",
    "print(mean_false_negatives.shape)\n",
    "mean_false_negatives = mean_false_negatives.reshape((25,25))\n",
    "\n",
    "print_heatmap(mean_false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGi8K_t5nbNK"
   },
   "source": [
    "Then we examine the cases in which the network should have responded _Background_, but it answered _Signal_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "1T79aGd7nbNL",
    "outputId": "82beb1da-8ab8-4867-f976-d8b668344ac2"
   },
   "outputs": [],
   "source": [
    "print(false_positives[1:10])\n",
    "false_positives_values = X_test.iloc[false_positives,]\n",
    "print(false_positives_values.shape)\n",
    "\n",
    "mean_false_positives = false_positives_values.mean().values\n",
    "print(mean_false_positives.shape)\n",
    "mean_false_positives = mean_false_positives.reshape((25,25))\n",
    "\n",
    "print_heatmap(mean_false_positives)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Image Jet Tagging Analysis (PTJ 250-300).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
